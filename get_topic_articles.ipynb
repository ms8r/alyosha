{
 "metadata": {
  "name": "get_topic_articles"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Retrieves articles for a given search string across sources and stores results as text files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from alyosha import alyosha as al\n",
      "reload(al)\n",
      "from alyosha import reference as REF\n",
      "import logging\n",
      "import os\n",
      "import time\n",
      "from random import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logging.basicConfig(filename='hobbylobby.log', level=logging.DEBUG)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_str = '\"hobby lobby\" supreme court decision'\n",
      "base_path = 'research/hl_texts'\n",
      "max_sleep = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "excludes = ['wsj.com', 'factcheck.org', 'fivethirtyeight.com']\n",
      "sources = [t[0] for t in REF.source_sites.values() if t[0] not in excludes]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for src in sources:\n",
      "    path = os.path.join(base_path, src)\n",
      "    if not os.path.isdir(path):\n",
      "        os.mkdir(path)\n",
      "    time.sleep(random() * max_sleep)\n",
      "    try:\n",
      "        sr = al.SiteResults(src, back_days = 90, search_str=search_str)\n",
      "    except al.EmptySearchResult:\n",
      "        logging.debug(\"EmptySearchResult for %s\", src)\n",
      "        continue\n",
      "    except al.ResultParsingError:\n",
      "        logging.debug(\"ResultParsingError for %s\", src)\n",
      "        continue\n",
      "    except al.PageRetrievalError:\n",
      "        logging.debug(\"PageRetrievalError for %s\", src)\n",
      "        continue\n",
      "    with open(os.path.join(path, \"result_info\"), 'w') as fout:\n",
      "        for i, r in enumerate(sr.res):\n",
      "            fout.write(\"*** %02d ***\\n\" % (i + 1)) \n",
      "            fout.write(\"%s\\n\" % r['title'].encode('utf-8'))\n",
      "            fout.write(\"%s\\n\" % r['desc'].encode('utf-8'))\n",
      "    for i, r in enumerate(sr.res):\n",
      "        time.sleep(random() * max_sleep)\n",
      "        try:\n",
      "            wa = al.WebArticle(r['url'], stop_words=REF.stop_words, late_kills=REF.late_kills)\n",
      "        except al.WebArticleError:\n",
      "            logging.debug(\"Couldn't extract article #%d from %s\", i, src)\n",
      "            continue\n",
      "        with open(os.path.join(path, \"a%02d.txt\" % (i + 1)), 'w') as fout:\n",
      "            fout.write(\"%s\\n\\n%s\" % (wa.title.encode('utf-8'), wa.text.encode('utf-8')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}